[user1@sms ~]$ sudo wwsh -y node new login --netdev eth0 \
    --ipaddr=172.16.0.2 --hwaddr=__:__:__:__:__:__
[user1@sms ~]$ sudo wwsh -y provision set login \
    --vnfs=rocky9.4 --bootstrap=$(uname -r) \
    --files=dynamic_hosts,passwd,group,shadow,munge.key,network
[user1@sms ~]$ sudo ssh login
[root@login ~]# df -h
Filesystem                Size  Used Avail Use% Mounted on
devtmpfs                  2.9G     0  2.9G   0% /dev
tmpfs                     2.9G  843M  2.1G  29% /
tmpfs                     2.9G     0  2.9G   0% /dev/shm
tmpfs                     1.2G  8.5M  1.2G   1% /run
172.16.0.1:/home           19G   12G  7.4G  61% /home
172.16.0.1:/opt/ohpc/pub  100G  6.0G   95G   6% /opt/ohpc/pub
tmpfs                     592M     0  592M   0% /run/user/0
[root@login ~]# sinfo
sinfo: error: resolve_ctls_from_dns_srv: res_nsearch error:
  Unknown host
sinfo: error: fetch_config: DNS SRV lookup failed
sinfo: error: _establish_config_source: failed to fetch config
sinfo: fatal: Could not establish a configuration source
[root@login ~]# sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 1-00:00:00      1   idle c[1-2]
ClusterName=cluster
SlurmctldHost=sms
ClusterName=cluster
SlurmctldHost=sms
[root@login ~]# sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 1-00:00:00      1   idle c[1-2]
[user1@sms ~]$ sudo scp login:/etc/slurm/slurm.conf \
  /etc/slurm/slurm.conf.login
slurm.conf                        100%   40    57.7KB/s   00:00
[user1@sms ~]$ sudo wwsh -y file import \
  /etc/slurm/slurm.conf.login --name=slurm.conf.login \
  --path=/etc/slurm/slurm.conf
[user1@sms ~]$ wwsh provision print c1
#### c1 ######################################################
 c1: MASTER           = UNDEF
 c1: BOOTSTRAP        = 6.1.96-1.el9.elrepo.x86_64
 c1: VNFS             = rocky9.4
 c1: VALIDATE         = FALSE
 c1: FILES            = dynamic_hosts,group,munge.key,network,
   passwd,shadow
...
 c1: KARGS            = "net.ifnames=0 biosdevname=0 quiet"
 c1: BOOTLOCAL        = FALSE
[user1@sms ~]$ wwsh provision print login
#### login ###################################################
 login: MASTER        = UNDEF
 login: BOOTSTRAP     = 6.1.96-1.el9.elrepo.x86_64
 login: VNFS          = rocky9.4
 login: VALIDATE      = FALSE
 login: FILES         = dynamic_hosts,group,munge.key,network,
   passwd,shadow
...
 login: KARGS         = "net.ifnames=0 biosdevname=0 quiet"
 login: BOOTLOCAL     = FALSE
wwsh provision print c1 | grep =
wwsh provision print c1 | grep = | cut -d: -f2-
wwsh provision print c1 | grep = | cut -d: -f2-
 MASTER           = UNDEF
 BOOTSTRAP        = 6.1.96-1.el9.elrepo.x86_64
 VNFS             = rocky9.4
 VALIDATE         = FALSE
 FILES            = dynamic_hosts,group,munge.key,network,
   passwd,shadow
...
 KARGS            = "net.ifnames=0 biosdevname=0 quiet"
 BOOTLOCAL        = FALSE
[user1@sms ~]$ function proprint() { \
  wwsh provision print $@ | grep = | cut -d: -f2- ; }
[user1@sms ~]$ proprint c1
 MASTER           = UNDEF
 BOOTSTRAP        = 6.1.96-1.el9.elrepo.x86_64
...
[user1@sms ~]$ diff -u <(proprint c1) <(proprint login)
[user1@sms ~]$
[user1@sms ~]$ sudo wwsh -y provision set login \
  --fileadd=slurm.conf.login
[user1@sms ~]$ diff -u <(proprint c1) <(proprint login)
--- /dev/fd/63  2024-07-06 11:11:07.682959677 -0400
+++ /dev/fd/62  2024-07-06 11:11:07.683959681 -0400
@@ -2,7 +2,7 @@
  BOOTSTRAP        = 6.1.96-1.el9.elrepo.x86_64
  VNFS             = rocky9.4
  VALIDATE         = FALSE
- FILES            = dynamic_hosts,group,munge.key,network,
  passwd,shadow
+ FILES            = dynamic_hosts,group,munge.key,network,
  passwd,shadow,slurm.conf.login
  PRESHELL         = FALSE
  POSTSHELL        = FALSE
  POSTNETDOWN      = FALSE
[user1@sms ~]$ sudo ssh login
[root@login ~]# systemctl edit slurmd
[Unit]
ConditionHost=|c*
ConditionHost=|g*
o slurmd.service - Slurm node daemon
...
  Condition: start condition failed at Sat 2024-07-06 18:12:17
    EDT; 4min 22s ago
...
Jul 06 17:14:16 login systemd[1]: Stopped Slurm node daemon.
Jul 06 18:12:17 login systemd[1]: Slurm node daemon was skipped
  because of an unmet condition check (ConditionHost=c*).
[user1@sms ~]$ sudo mkdir -p \
  ${CHROOT}/etc/systemd/system/slurmd.service.d/
[user1@sms ~]$ sudo scp \
  login:/etc/systemd/system/slurmd.service.d/override.conf \
  ${CHROOT}/etc/systemd/system/slurmd.service.d/
override.conf                    100%   23    36.7KB/s   00:00
[user1@sms ~]$ sudo wwvnfs --chroot=${CHROOT}
Using 'rocky9.4' as the VNFS name
...
Total elapsed time                                          : 84.45 s
[user1@sms ~]$ sudo ssh login reboot
[user1@sms ~]$ sudo ssh c1 reboot
[user1@sms ~]$ sudo ssh login systemctl status slurmd
o slurmd.service - Slurm node daemon
...
Jul 06 18:26:23 login systemd[1]: Slurm node daemon was
  skipped because of an unmet condition check
  (ConditionHost=c*).
[user1@sms ~]$ sudo ssh login sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 1-00:00:00      1   idle c[1-2]
[user1@sms ~]$ sudo ssh c1 systemctl status slurmd
o slurmd.service - Slurm node daemon
...
Jul 06 19:03:22 c1 slurmd[1082]: slurmd: CPUs=2 Boards=1 
  Sockets=2 Cores=1 Threads=1 Memory=5912 TmpDisk=2956
  Uptime=28 CPUSpecList=(null) FeaturesAvail=(null)
  FeaturesActive=(null)
[user1@sms ~]$ sudo ssh c1 sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 1-00:00:00      1   idle c2
normal*      up 1-00:00:00      1   down c1
[user1@sms ~]$ ssh login
Access denied: user user1 (uid=1001) has no active jobs on this
  node.
Connection closed by 172.16.0.2 port 22
[user1@sms ~]$ sudo wwsh -y file import /etc/pam.d/sshd \
  --name=sshd.login
[user1@sms ~]$ wwsh file list
...
sshd.login :  rw-r--r-- 1   root root      727 /etc/pam.d/sshd
[user1@sms ~]$ sudo wwsh -y provision set login \
  --fileadd=sshd.login
[user1@sms ~]$ diff -u <(proprint c1) <(proprint login)
...
  VALIDATE         = FALSE
- FILES            = dynamic_hosts,group,munge.key,network,
  passwd,shadow
+ FILES            = dynamic_hosts,group,munge.key,network,
  passwd,shadow,slurm.conf.login,sshd.login
...
[user1@sms ~]$ sudo ssh login reboot
[user1@sms ~]$ ssh login
[user1@login ~]$
Jul 11 11:24:06 sms sshd[162636]: Invalid user evilmike from 
  68.66.205.120 port 1028
...
Jul 11 11:24:08 sms sshd[162636]: Failed password for invalid
  user evilmike from 68.66.205.120 port 1028 ssh2
...
[user1@sms ~]$ sudo ssh login netstat -utlp | grep -v localhost
Active Internet connections (only servers)
Proto ... Local Address  ... State      PID/Program name
tcp       0.0.0.0:ssh        LISTEN     1034/sshd: /usr/sbi
tcp       0.0.0.0:sunrpc     LISTEN     1/init
tcp6      [::]:ssh           LISTEN     1034/sshd: /usr/sbi
tcp6      [::]:sunrpc        LISTEN     1/init
udp       0.0.0.0:sunrpc     0.0.0.0:*  1/init
udp       0.0.0.0:37036      0.0.0.0:*  1143/rsyslogd
udp6      [::]:sunrpc        [::]:*     1/init
[user1@sms ~]$ sudo yum install --installroot=${CHROOT} \
  fail2ban
[user1@sms ~]$ sudo chroot ${CHROOT} systemctl enable \
  fail2ban firewalld
[sshd]
enabled = true
[user1@sms ~]$ sudo mkdir -p \
  ${CHROOT}/etc/systemd/system/fail2ban.service.d/ \
  ${CHROOT}/etc/systemd/system/firewalld.service.d/
[user1@sms ~]$ sudo nano \
  ${CHROOT}/etc/systemd/system/fail2ban.service.d/override.conf
[Unit]
ConditionHost=|login*
[user1@sms ~]$ sudo cp \
${CHROOT}/etc/systemd/system/fail2ban.service.d/override.conf \
${CHROOT}/etc/systemd/system/firewalld.service.d/override.conf
[user1@sms ~]$ sudo ssh login ls -l /var/log/secure
-rw------- 1 root root 0 Jul  7 03:14 /var/log/secure
[user1@sms ~]$ echo "authpriv.* /var/log/secure" | \
  sudo tee ${CHROOT}/etc/rsyslog.d/authpriv-local.conf
authpriv.* /var/log/secure
[user1@sms ~]$ cat \
  ${CHROOT}/etc/rsyslog.d/authpriv-local.conf
authpriv.* /var/log/secure
[user1@sms ~]$ sudo wwvnfs --chroot=${CHROOT}
[user1@sms ~]$ sudo ssh login reboot
[user1@sms ~]$ sudo ssh login systemctl status firewalld
[root@login ~]# systemctl status firewalld
x firewalld.service - firewalld - dynamic firewall daemon
     Loaded: loaded (/usr/lib/systemd/system/firewalld.service;
       enabled; preset>
     Active: failed (Result: exit-code) since Thu 2024-07-11
       16:49:47 EDT; 46mi>
...
Jul 11 16:49:47 login systemd[1]: firewalld.service: Main
  process exited, code=exited, status=3/NOTIMPLEMENTED
Jul 11 16:49:47 login systemd[1]: firewalld.service: Failed
  with result 'exit-code'.
[user1@sms ~]$ grep kernel/net /etc/warewulf/bootstrap.conf
[user1@sms ~]$
[user1@sms ~]$ grep kernel/net /etc/warewulf/bootstrap.conf
[user1@sms ~]$ echo "drivers += kernel/net/" | \
  sudo tee -a /etc/warewulf/bootstrap.conf
drivers += kernel/net/
[user1@sms ~]$ grep kernel/net /etc/warewulf/bootstrap.conf
drivers += kernel/net/
[user1@sms ~]$ sudo wwbootstrap $(uname -r)
...
Bootstrap image '6.1.97-1.el9.elrepo.x86_64' is ready
Done.
[user1@sms ~]$ sudo ssh login reboot
[user1@sms ~]$ sudo ssh login systemctl status firewalld
o firewalld.service - firewalld - dynamic firewall daemon
     Loaded: loaded (/usr/lib/systemd/system/firewalld.service;
       enabled; preset: enabled)
     Active: active (running) since Thu 2024-07-11 21:58:18
       EDT; 43s ago
...
Jul 11 21:58:18 login systemd[1]: Starting firewalld - dynamic
  firewall daemon...
Jul 11 21:58:18 login systemd[1]: Started firewalld - dynamic 
  firewall daemon.
[user1@sms ~]$ sudo ssh login grep 68.66.205.120 \
  /var/log/fail2ban.log
...
2024-07-11 22:02:27,030 fail2ban.actions ... [sshd] Ban \
  68.66.205.120
mike@server:~$ ssh evilmike@149.165.155.235
evilmike@149.165.155.235's password:
Permission denied, please try again.
evilmike@149.165.155.235's password:
Permission denied, please try again.
evilmike@149.165.155.235's password:
evilmike@149.165.155.235: Permission denied (publickey,
  gssapi-keyex,gssapi-with-mic,password).
mike@server:~$ ssh evilmike@149.165.155.235
ssh: connect to host 149.165.155.235 port 22: Connection
  refused
[user1@sms ~]$ sudo scontrol update node=c1 state=resume
[user1@sms ~]$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 1-00:00:00      1   idle c[1-2]
[user1@sms ~]$ grep -i reboot /etc/slurm/slurm.conf
#RebootProgram=
[user1@sms ~]$ echo 'RebootProgram="/sbin/shutdown -r now"' \
  | sudo tee -a /etc/slurm/slurm.conf
[user1@sms ~]$ grep -i reboot /etc/slurm/slurm.conf
#RebootProgram=
RebootProgram="/sbin/shutdown -r now"
[user1@sms ~]$ sudo scontrol reconfigure
[user1@sms ~]$ sudo scontrol reboot ASAP nextstate=RESUME c1
[user1@sms ~]$ sudo ssh c1 uptime
 15:52:27 up 1 min,  0 users,  load average: 0.09, 0.06, 0.02
[user1@sms ~]$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 1-00:00:00      1   idle c[1-2]
[user1@sms ~]$ sudo ssh c1
[root@c1 ~]# df -h /tmp
Filesystem      Size  Used Avail Use% Mounted on
tmpfs           2.9G  843M  2.1G  29% /
[root@c1 ~]# free -m
               total        used        free ...
Mem:            5912        3162        2862 ...
Swap:              0           0           0 ...
[root@c1 ~]# module load py3-numpy
[root@c1 ~]# python3 -c \
  'import numpy as np; x=np.full((25000, 25000), 1)'
[root@c1 ~]#
[root@c1 ~]# dd if=/dev/zero of=/tmp/foo bs=1M count=1024
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.63492 s, 1.7 GB/s
[root@c1 ~]# module load py3-numpy
[root@c1 ~]# python3 -c \
  'import numpy as np; x=np.full((25000, 25000), 1)'
Killed
[root@c1 ~]# rm /tmp/foo
[root@c1 ~]# python3 -c \
  'import numpy as np; x=np.full((25000, 25000), 1)'
[root@c1 ~]# exit
[user1@sms ~]$ 
[user1@sms ~]$ sudo ssh c1 parted -l /dev/vda
Model: Virtio Block Device (virtblk)
Disk /dev/vda: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start   End     Size    File system     Name  Flags
 1      1049kB  3146kB  2097kB                  EFI   boot, esp
[user1@sms ~]$ sudo nano \
  /etc/warewulf/filesystem/jetstream.cmds
select /dev/vda
# mkpart primary 3MiB 515MiB
# mkpart primary 515MiB 2563MiB
# mkpart primary 2563MiB 4611MiB
# mkpart primary 4611MiB 100%
name 2 boot
name 3 swap
name 4 root
name 5 tmp
## mkfs NUMBER FS-TYPE [ARGS...]
mkfs 2 ext4 -L boot
mkfs 3 swap
mkfs 4 ext4 -L root
mkfs 5 ext4 -L tmp
## fstab NUMBER mountpoint type opts freq passno
fstab 4 /     ext4 defaults 0 0
fstab 2 /boot ext4 defaults 0 0
fstab 3 none  swap defaults 0 0
fstab 5 /tmp  ext4 defaults 0 0
[user1@sms ~]$ grep mkpart \
  /etc/warewulf/filesystem/jetstream.cmds
# mkpart primary 3MiB 515MiB
# mkpart primary 515MiB 2663MiB
# mkpart primary 2663MiB 4611MiB
# mkpart primary 4611MiB 100%
[user1@sms ~]$ grep mkpart \
  /etc/warewulf/filesystem/jetstream.cmds | sed 's/#//g'
 mkpart primary 3MiB 515MiB
 mkpart primary 515MiB 2663MiB
 mkpart primary 2663MiB 4611MiB
 mkpart primary 4611MiB 100%
[user1@sms ~]$ echo $(grep mkpart \
  /etc/warewulf/filesystem/jetstream.cmds | sed 's/#//g')
mkpart primary 3MiB 515MiB mkpart primary 515MiB 2663MiB mkpart primary ext4 2663MiB 4611MiB mkpart
  primary ext4 4611MiB 100%
[user1@sms ~]$ sudo ssh c1 parted --script /dev/vda
  $(echo $(grep mkpart
  /etc/warewulf/filesystem/jetstream.cmds |
  sed 's/#//g'))
[user1@sms ~]$ sudo ssh c1 parted -l
Model: Virtio Block Device (virtblk)
Disk /dev/vda: 64.4GB
...
Number Start  End    Size   File system Name    Flags
 1     1049kB 3146kB 2097kB             EFI     boot, esp
 2     3146kB 540MB  537MB              primary
 3     540MB  2688MB 2147MB             primary swap
 4     2688MB 4835MB 2147MB             primary
 5     4835MB 21.5GB 16.6GB             primary
[user1@sms ~]$ sudo wwsh provision set 'c*' \
  --filesystem=jetstream
[user1@sms ~]$ echo modprobe += virtio_blk | \
  sudo tee -a /etc/warewulf/bootstrap.conf
[user1@sms ~]$ echo modprobe += ext4 | \
  sudo tee -a /etc/warewulf/bootstrap.conf
[user1@sms ~]$ sudo wwbootstrap $(uname -r)
[user1@sms ~]$ sudo scontrol reboot ASAP nextstate=RESUME \
  c[1-2]
[user1@sms ~]$ sudo ssh c1 "df -h; free -m"
Filesystem                Size  Used Avail Use% Mounted on
devtmpfs                  2.9G     0  2.9G   0% /dev
/dev/vda4                 1.9G  853M  914M  49% /
tmpfs                     2.9G     0  2.9G   0% /dev/shm
tmpfs                     1.2G  8.5M  1.2G   1% /run
/dev/vda2                 488M   40K  452M   1% /boot
/dev/vda5                  16G   72K   15G   1% /tmp
...
       total  used  free  shared  buff/cache  available
Mem:    5912   382  4844       8         939       5530
Swap:   2047     0  2047
[user1@sms ~]$ sudo ssh c1
[root@c1 ~]# dd if=/dev/zero of=/tmp/foo bs=1M count=5120
5120+0 records in
5120+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 8.97811 s, 598 MB/s
[root@c1 ~]# module load py3-numpy
[root@c1 ~]# python3 -c \
  'import numpy as np; x=np.full((25000, 25000), 1)'
[root@c1 ~]# rm /tmp/foo
[user1@sms ~]$ wwbootstrap --help
USAGE: /usr/bin/wwbootstrap [options] kernel_version
...
    OPTIONS:
        -c, --chroot  Look into this chroot directory to find
                      the kernel
...
[user1@sms ~]$ sudo yum -y install --installroot=$CHROOT kernel
...
Installing:
 kernel  x86_64 5.14.0-427.24.1.el9_4 ...
...
Complete!
[user1@sms ~]$ sudo wwbootstrap --chroot=${CHROOT} \
  5.14.0-427.24.1.el9_4.x86_64
Number of drivers included in bootstrap: 880
...
Bootstrap image '5.14.0-427.24.1.el9_4.x86_64' is ready
Done.
[user1@sms ~]$ wwsh provision list
NODE                VNFS            BOOTSTRAP         ...    
=========================================================
c1                  rocky9.4        6.1.97-1.el9.elrep...
c2                  rocky9.4        6.1.97-1.el9.elrep...
login               rocky9.4        6.1.97-1.el9.elrep...
[user1@sms ~]$ sudo wwsh provision set '*' \
  --bootstrap=5.14.0-427.24.1.el9_4.x86_64
Are you sure you want to make the following changes to 3
  node(s):

     SET: BOOTSTRAP            = 5.14.0-427.24.1.el9_4.x86_64

Yes/No> y
[user1@sms ~]$ sudo scontrol reboot ASAP nextstate=RESUME \
  c[1-2]
[user1@sms ~]$ sudo pdsh -w 'login' reboot
[user1@sms ~]$ sudo pdsh -w 'c[1-2],login' uname -r \
  | sort
c1: 5.14.0-427.24.1.el9_4.x86_64
c2: 5.14.0-427.24.1.el9_4.x86_64
login: 5.14.0-427.24.1.el9_4.x86_64
[user1@sms ~]$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 1-00:00:00      2   idle c[1-2]
[renfro2@sms ~]$ sudo ssh gpunode002 lspci | grep -i nvidia
05:00.0 ... NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
06:00.0 ... NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
84:00.0 ... NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
85:00.0 ... NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
[renfro2@sms ~]$ sudo ssh gpunode002 nvidia-smi | grep Driver
| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   ...
[renfro2@sms ~]$ sudo ssh gpunode002 "uname -r"
4.18.0-513.24.1.el8_9.x86_64
[renfro2@sms ~]$ KV=$(sudo ssh gpunode002 "uname -r")
[renfro2@sms ~]$ NV=470.256.02
[renfro2@sms ~]$ BASEURL=https://us.download.nvidia.com/tesla
[renfro2@sms ~]$ wget \
  ${BASEURL}/${NV}/NVIDIA-Linux-x86_64-${NV}.run
...
... 'NVIDIA-Linux-x86_64-470.256.02.run' saved ...
[renfro2@sms ~]$
[renfro2@sms ~]$ sudo install -o root -g root -m 0755 \
  NVIDIA-Linux-x86_64-${NV}.run ${CHROOT}/root
[renfro2@sms ~]$ sudo mount -o rw,bind /proc ${CHROOT}/proc
[renfro2@sms ~]$ sudo mount -o rw,bind /dev ${CHROOT}/dev
[user1@sms ~]$ sudo chroot ${CHROOT} \
  /root/NVIDIA-Linux-x86_64-${NV}.run --disable-nouveau \
  --kernel-name=${KV} --no-drm --run-nvidia-xconfig --silent
[renfro2@sms ~]$ sudo rm \
  ${CHROOT}/root/NVIDIA-Linux-x86_64-${NV}.run
[renfro2@sms ~]$ sudo umount ${CHROOT}/proc ${CHROOT}/dev
[renfro2@sms ~]$ sudo wwvnfs --chroot=${CHROOT}
[renfro2@sms ~]$ wwsh provision print gpunode002 | \
  egrep -i 'bootstrap|vnfs'
    gpunode002: BOOTSTRAP        = 4.18.0-513.24.1.el8_9.x86_64
    gpunode002: VNFS             = rocky-8-k80
[renfro2@sms ~]$ sudo ssh gpunode002 reboot
[renfro2@sms ~]$ sudo ssh gpunode002 uptime
 15:11:05 up 1 min,  0 users,  load average: 1.56, 0.51, 0.18
[renfro2@sms ~]$ sudo ssh gpunode002 lspci | grep -i nvidia
05:00.0 ... NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
06:00.0 ... NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
84:00.0 ... NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
85:00.0 ... NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
[renfro2@sms ~]$ sudo ssh gpunode002 nvidia-smi | grep Driver
| NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   ...
# These could be saved into a common settings file
CHROOTS = {
  None:   ['/opt/ohpc/admin/images/rocky-cpu', None],
  'a100': ['/opt/ohpc/admin/images/rocky-a100', '550.90.07'],
  'k80':  ['/opt/ohpc/admin/images/rocky-k80', '470.256.02'],
  }
NV_BASEURL = 'https://us.download.nvidia.com/tesla'
KERNEL_VER = '4.18.0-513.5.1.el8_9.x86_64'
NV_OPTS = " ".join([f"--disable-nouveau",
                    f"--kernel-name={KERNEL_VER}",
                    f"--no-drm",
                    f"--run-nvidia-xconfig",
                    f"--silent",
                    ])
# This could be a function or a script to install GPU drivers
import os
for gpu in CHROOTS.keys():
  if gpu == None:
    continue
  chroot, version = CHROOTS[gpu]
  driver = f"NVIDIA-Linux-x86_64-{version}.run"
  os.system(f"curl -sLO {NV_BASEURL}/{version}/{driver}")
  os.system(f"install -o root -g root {driver} {chroot}/root/")
  # to be continued...
  # continued...
  for mnt in ['proc', 'dev']:
    os.system(f"mount -o rw,bind /{mnt} {chroot}/{mnt}")
  os.system(f"chroot {chroot} /root/{driver} {driver_opts}")
  for mnt in ['proc', 'dev']:
    os.system(f"umount {chroot}/{mnt}")
  os.remove(f"{chroot}/root/{driver}")
NODES = [
  { 'hostname': 'gpunode012',
    'eth_dev': 'eth2', 'eth_ip': '149.149.248.204/25',
    'eth_dev_mac': 'B0:7B:25:DE:68:26',
    'ib_dev': 'ib0', 'ib_ip': '172.16.1.204/23',
    'stateful': '2-drive', 'gpu': 'a100', },
  { 'hostname': 'login', 'role': 'login',
    'eth_dev': 'eth0', 'eth_ip': '149.149.248.135/25',
    'eth_dev_mac': '00:50:56:81:50:72',
    'extra_interfaces': { 'eth1': '10.10.25.126/21' }, },
  ]
file_list = ["dynamic_hosts", "passwd", "group", "shadow",
             "munge.key",]
for node in NODES:
  fileadd_list = []
  fileadd_list.append(f"network.{node['eth_dev']}")
  if 'extra_interfaces' in node:
    for dev, ip in node.extra_interfaces.items():
      ip_mask = ipaddress.ip_interface(ip)
      fileadd_list.append(f"ifcfg-{dev}.ww")
      os.system(f"wwsh -y node set {node['hostname']}"
                f"-D {dev} --ipaddr={ip_mask.ip}"
                f"--netmask={ip_mask.netmask}")
  os.system(f"wwsh -y provision set {node['hostname']}"
            f"--files={','.join(file_list)}"
            f"--fileadd={','.join(fileadd_list)}")
  #SBATCH --account=member1
  #SBATCH --account=paygo-project1 --qos=paygo-project1
[user1@sms ~]$ sudo mysqladmin create slurm_acct_db
[user1@sms ~]$ sudo mysql
MariaDB [(none)]> create user 'slurm'@'localhost' identified
  by 'some_other_password';
MariaDB [(none)]> grant all privileges on slurm_acct_db.* to
  'slurm'@'localhost';
MariaDB [(none)]> exit
[user1@sms ~]$ sudo nano /etc/my.cnf.d/slurmdbd.cnf
[mysqld]
innodb_lock_wait_timeout=900
innodb_buffer_pool_size=4096M
[user1@sms ~]$ sudo systemctl restart mariadb
[user1@sms ~]$ sudo cp /etc/slurm/slurmdbd.conf.example \
  /etc/slurm/slurmdbd.conf
[user1@sms ~]$ sudo nano /etc/slurm/slurmdbd.conf
[user1@sms ~]$ sudo systemctl restart slurmdbd
[user1@sms ~]$ sudo nano /etc/slurm/slurm.conf
[user1@sms ~]$ sudo scontrol reconfigure
[user1@sms ~]$ sudo sacctmgr add cluster 'cluster'
[user1@sms ~]$ srun hostname
[user1@sms ~]$ sacct
JobID           JobName  Partition ...
------------ ---------- ---------- ...
...            hostname     normal ...
[user1@sms ~]$ sudo sacctmgr add account member1 \
    cluster=cluster Description="Member1 Description" \
    FairShare=N
[user1@sms ~]$ sudo sacctmgr add user user1 account=member1
[user1@sms ~]$ sudo sacctmgr remove user where user=user1 \
  and account=member1
[user1@sms ~]$ sudo sacctmgr modify account member1 set \
  FairShare=N
[user1@sms ~]$ sudo sacctmgr add user user1 account=member1 \
  partition=gpu
[user1@sms ~]$ sudo sacctmgr modify user user1 set \
  FairShare=N where account=member1 partition=gpu
[user1@sms ~]$ sacctmgr add account paygo cluster=cluster \
  Description="PAYGO Projects" FairShare=N
[user1@sms ~]$ sacctmgr add account paygo-project1 \
  cluster=cluster Description="PAYGO Project 1" parent=paygo
[user1@sms ~]$ sacctmgr add qos paygo-project1 \
  flags=NoDecay,DenyOnLimit
[user1@sms ~]$ sacctmgr modify qos paygo-project1 set \
  grptresmins=cpu=1000
[user1@sms ~]$ sacctmgr modify account name=paygo set \
  qos+=paygo-project1
sacctmgr modify account paygo set FairShare=N
scontrol -o show assoc_mgr qos=paygo-project1 | \
    grep QOS=paygo-project1 | egrep -o 'UsageRaw=[0-9.]*'
sacctmgr modify user user1 set qos+=paygo-project1
sacctmgr modify user user1 set qos-=paygo-project1
sacctmgr modify account name=paygo-project1 set \
  qos-=paygo-project1
sacctmgr remove qos paygo-project1
sacctmgr remove account paygo-project1
sacctmgr add account gratis cluster=cluster \
    Description="Gratis Usage" FairShare=N
sacctmgr add user username DefaultAccount=gratis
sacctmgr modify account gratis set FairShare=N
PartitionName=DEFAULT ExclusiveUser=NO LLN=NO MinNodes=1
  PriorityJobFactor=1
PartitionName=interactive
MaxNodes=4
DefMemPerCPU=2000
DefaultTime=02:00:00
MaxTime=02:00:00
AllowAccounts=ALL
PriorityTier=3
Nodes=c[1-2]
PartitionName=debug
DefMemPerCPU=2000
DefaultTime=00:30:00
MaxTime=00:30:00
AllowAccounts=ALL 
PriorityTier=2
Nodes=c[1-2]
PartitionName=batch
Default=YES
MaxNodes=2
DefMemPerCPU=2000
DefaultTime=06:00:00
MaxTime=2-00:00:00
AllowAccounts=ALL
PriorityTier=1
Nodes=c[1-2]
PartitionName=long
MaxNodes=40
DefMemPerCPU=2000
DefaultTime=1-00:00:00
MaxTime=7-00:00:00
DenyAccounts=gratis
PriorityTier=2
Nodes=node[001-040]
